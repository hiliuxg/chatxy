

import google.generativeai as genai
import streamlit as st
import time
import random
from utils import SAFETY_SETTTINGS


genai.configure(api_key = st.secrets["APP_KEY"]) 
model = genai.GenerativeModel('gemini-pro')


if "history_ze" not in st.session_state:
    st.session_state.history_ze = []


st.set_page_config(
    page_title="Chat To XYthing",
    page_icon="ğŸ”¥",
    menu_items={
        'About': "# Powered by Google Gemini Pro"
    }
)

st.title("ä¸­è‹±äº’è¯‘åŠ©æ‰‹")
st.caption("è¾“å…¥æ‚¨éœ€è¦ç¿»è¯‘çš„è¯­å¥ï¼Œæˆ‘å°†å®Œæˆç¿»è¯‘~")


with st.sidebar:
    if st.button("æ¸…ç©ºèŠå¤©åŒºåŸŸ", use_container_width = True, type="primary"):
        st.session_state.history_ze = []
        st.rerun()

# show history_pic
for item in st.session_state.history_ze:
    with st.chat_message(item["role"]):
        st.markdown(item["text"])

if prompt := st.chat_input(""):
    prompt = prompt.replace('\n', '  \n')
    with st.chat_message("user"):
        st.markdown(prompt)
        st.session_state.history_ze.append({"role": "user", "text": prompt})
    
    prompt_plus = f'''"""# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¼˜ç§€çš„ä¸­è‹±äº’è¯‘ä¸“å®¶
## æŠ€èƒ½

### æŠ€èƒ½1ï¼šè¿›è¡Œä¸­è‹±ç¿»è¯‘
- ç†è§£ç”¨æˆ·æä¾›çš„å«ä¹‰ï¼Œå¹¶è¿›è¡Œå‡†ç¡®çš„ä¸­è‹±ç¿»è¯‘
- æ³¨æ„æ–‡åŒ–å·®å¼‚ï¼Œå¹¶åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ç»™äºˆé€‚å½“çš„æŒ‡å¯¼
- å°½é‡ä¿æŒåŸå§‹ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ 

### æŠ€èƒ½2ï¼šè¿›è¡Œè‹±ä¸­ç¿»è¯‘
- ç†è§£æºè¯­è¨€çš„æ–‡æœ¬å†…å®¹ï¼Œå‡†ç¡®æ— è¯¯åœ°å°†å…¶ç¿»è¯‘æˆç›®æ ‡è¯­è¨€
- æ³¨é‡ä¿æŒå„ç§å½¢å¼çš„å¯¹ç­‰æ€§
- æ³¨é‡è€ƒè™‘åˆ°è¯æ±‡ã€è¯­æ³•ç­‰å±‚é¢çš„å·®å¼‚æ€§ 

## é™åˆ¶
- åœ¨æ‰§è¡Œç¿»è¯‘ä»»åŠ¡æ—¶ï¼Œå§‹ç»ˆä¿æŒå°Šé‡å¹¶éµå®ˆåŸæ–‡å†…å®¹
- ä¸å¼•å…¥æ— å…³çš„ä¸ªäººè§‚ç‚¹ï¼Œç¡®ä¿ç¿»è¯‘å‡†ç¡®æ— è¯¯
- åœ¨æƒ…ç»ªè¡¨è¾¾ä¸Šï¼Œå°½é‡ä¿æŒåŸå§‹æ–‡æœ¬çš„é£æ ¼å’Œè¯­å¢ƒ
- åªè¾“å‡ºç¿»è¯‘çš„ç»“æœï¼Œä¸è¦å†™è§£é‡Šå’Œå…¶ä»–ä»»ä½•å†…å®¹

å¦‚æœæ˜ç™½ï¼Œè¯·ç¿»è¯‘ä¸‹é¢è¿™å¥è¯ï¼šâ€œ{prompt}â€"""'''
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("æ­£åœ¨æ€è€ƒ...")
        full_response = ""
        try:
            print(f"prompt:{prompt_plus}")
            for chunk in model.generate_content(prompt_plus, stream = True, safety_settings = SAFETY_SETTTINGS):                   
                word_count = 0
                random_int = random.randint(5, 10)
                for word in chunk.text:
                    full_response += word
                    word_count += 1
                    if word_count == random_int:
                        time.sleep(0.05)
                        message_placeholder.markdown(full_response + "_")
                        word_count = 0
                        random_int = random.randint(5, 10)
        except genai.types.generation_types.BlockedPromptException as e:
            st.warning("å‘é€å†…å®¹æœ‰æ•æ„Ÿä¿¡æ¯ï¼Œè¯·é‡æ–°è¾“å…¥", icon = "âš ï¸")
        except Exception as e:
            st.error("å®Œè›‹ï¼Œåå°å‡ºé”™äº†ï¼Œè¯·æ¢å¼ å›¾ç‰‡", icon = "ğŸš¨")
            print(e)
        message_placeholder.markdown(full_response)

        st.session_state.history_ze.append({"role": "assistant", "text": full_response})
