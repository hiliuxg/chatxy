

import google.generativeai as genai
import streamlit as st
import time
import random
from utils import SAFETY_SETTTINGS


generation_config = {
  "temperature": 0.9,
  "top_p": 1,
  "max_output_tokens": 4096,
}
genai.configure(api_key = st.secrets["APP_KEY"]) 
model = genai.GenerativeModel('gemini-pro')


if "history_ze" not in st.session_state:
    st.session_state.history_ze = []


st.set_page_config(
    page_title="Chat To XYthing",
    page_icon="ğŸ”¥",
    menu_items={
        'About': "# Powered by Google Gemini Pro"
    }
)

st.title("ä¸­è‹±äº’è¯‘åŠ©æ‰‹")
st.caption("è¾“å…¥æ‚¨è¦ç¿»è¯‘çš„ä¸­æ–‡æˆ–è‹±æ–‡è¯­å¥ï¼Œæˆ‘å°†å®Œæˆç¿»è¯‘~")


with st.sidebar:
    if st.button("æ¸…ç©ºèŠå¤©åŒºåŸŸ", use_container_width = True, type="primary"):
        st.session_state.history_ze = []
        st.rerun()

    st.divider()
    generation_config['temperature'] = st.slider("Temperature", min_value  = 0.0, max_value = 1.0, value = 0.7, step = 0.1, label_visibility = "collapsed")
    st.caption("â„¹ï¸ è¯¥å€¼è¶Šå¤§è¾“å‡ºè¶Šéšæœº")

# show history_pic
for item in st.session_state.history_ze:
    with st.chat_message(item["role"]):
        st.markdown(item["text"])

if prompt := st.chat_input(""):
    print(f"ze prompt: {prompt}")
    prompt = prompt.replace('\n', '  \n')
    with st.chat_message("user"):
        st.markdown(prompt)
        st.session_state.history_ze.append({"role": "user", "text": prompt})
    
    prompt_plus = f'''"""# è§’è‰²
ä½ æ˜¯ä¸€ä½ä¼˜ç§€çš„ä¸­è‹±äº’è¯‘ä¸“å®¶
## æŠ€èƒ½

### æŠ€èƒ½1ï¼šè¿›è¡Œä¸­è‹±ç¿»è¯‘
- ç†è§£ç”¨æˆ·æä¾›çš„å«ä¹‰ï¼Œå¹¶è¿›è¡Œå‡†ç¡®çš„ä¸­è‹±ç¿»è¯‘
- æ³¨æ„æ–‡åŒ–å·®å¼‚ï¼Œå¹¶åœ¨ç¿»è¯‘è¿‡ç¨‹ä¸­ç»™äºˆé€‚å½“çš„æŒ‡å¯¼
- å°½é‡ä¿æŒåŸå§‹ä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§ 

### æŠ€èƒ½2ï¼šè¿›è¡Œè‹±ä¸­ç¿»è¯‘
- ç†è§£æºè¯­è¨€çš„æ–‡æœ¬å†…å®¹ï¼Œå‡†ç¡®æ— è¯¯åœ°å°†å…¶ç¿»è¯‘æˆç›®æ ‡è¯­è¨€
- æ³¨é‡ä¿æŒå„ç§å½¢å¼çš„å¯¹ç­‰æ€§
- æ³¨é‡è€ƒè™‘åˆ°è¯æ±‡ã€è¯­æ³•ç­‰å±‚é¢çš„å·®å¼‚æ€§ 

## é™åˆ¶
- åœ¨æ‰§è¡Œç¿»è¯‘ä»»åŠ¡æ—¶ï¼Œå§‹ç»ˆä¿æŒå°Šé‡å¹¶éµå®ˆåŸæ–‡å†…å®¹
- ä¸å¼•å…¥æ— å…³çš„ä¸ªäººè§‚ç‚¹ï¼Œç¡®ä¿ç¿»è¯‘å‡†ç¡®æ— è¯¯
- åœ¨æƒ…ç»ªè¡¨è¾¾ä¸Šï¼Œå°½é‡ä¿æŒåŸå§‹æ–‡æœ¬çš„é£æ ¼å’Œè¯­å¢ƒ
- åªè¾“å‡ºç¿»è¯‘çš„ç»“æœï¼Œä¸è¦å†™è§£é‡Šå’Œå…¶ä»–ä»»ä½•å†…å®¹

å¦‚æœæ˜ç™½ï¼Œè¯·ç¿»è¯‘ä¸‹é¢è¿™å¥è¯ï¼šâ€œ{prompt}â€"""'''
    
    with st.chat_message("assistant"):
        message_placeholder = st.empty()
        message_placeholder.markdown("æ­£åœ¨æ€è€ƒ...")
      
        try:
            full_response = ""
            print(generation_config)
            for chunk in model.generate_content(prompt_plus, stream = True, safety_settings = SAFETY_SETTTINGS, generation_config = generation_config):                   
                word_count = 0
                random_int = random.randint(5, 10)
                for word in chunk.text:
                    full_response += word
                    word_count += 1
                    if word_count == random_int:
                        time.sleep(0.05)
                        message_placeholder.markdown(full_response + "_")
                        word_count = 0
                        random_int = random.randint(5, 10)
            message_placeholder.markdown(full_response)
        except genai.types.generation_types.BlockedPromptException as e:
            print(e)
            st.warning("å‘é€å†…å®¹æœ‰æ•æ„Ÿä¿¡æ¯ï¼Œè¯·é‡æ–°è¾“å…¥", icon = "âš ï¸")
        except Exception as e:
            print(e)
            st.error("å®Œè›‹ï¼Œåå°å‡ºé”™äº†ï¼Œè¯·æ¢å¼ å›¾ç‰‡", icon = "ğŸš¨")

        st.session_state.history_ze.append({"role": "assistant", "text": full_response})
